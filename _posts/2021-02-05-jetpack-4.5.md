---
layout: post
comments: true
title: "JetPack-4.5"
excerpt: "This post summarizes how I set up my Jetson DevKits and run my tensorrt_demos samples."
date: 2021-02-05
category: "jetson"
tags: jetson nano xavier-nx
---

This is a straightforward follow-up of my previos [JetPack-4.4 for Jetson Nano](https://jkjung-avt.github.io/jetpack-4.4/) and [Setting up Jetson Xavier NX](https://jkjung-avt.github.io/setting-up-xavier-nx/) posts.  As NVIDIA formally released [JetPack-4.5](https://forums.developer.nvidia.com/t/jetpack-4-5-production-release-with-l4t-32-5/166475) a couple of weeks ago, I have tested it and documented the steps of installing various software packages and testing my [tensorrt_demos](https://github.com/jkjung-avt/tensorrt_demos) examples.

So far I have only tested JetPack-4.5 on my Jetson Nano DevKit.  But I believe the following steps work equally well on Jetson Xavier NX and else.

# 1. Basic set-up (microSD card)

I recommend using a microSD card of at least 128GB in size.

I downloaded the JetPack-4.5 image, "Jetson Nano Developer Kit SD Card Image", from the official [Getting Started With Jetson Nano Developer Kit](https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#write) page and "etched" the image onto my microSD card.  I then inserted the microSD card and boot my Jetson Nano DevKit.  During initial Ubuntu set-up, I selected "MAXN" mode for the Nano.  And I created an acccount named "nvidia" (you could replace it with your preferred user name).

> Tip #1:  When the Jetson Nano boot up to the Desktop, I would hit Ctrl-Alt-T to bring up a terminal and then lock the terminal on the Launcher (on the left-hand side).

> Tip #2:  In the command terminal, I would type and execute "chromium-browser" to bring up the web browser.  Again, I would lock it onto Launcher for easier use later on.

Then I'd run the following commands to set up CUDA related environment variables properly.

```shell
$ sudo jetson_clocks
$ sudo apt update
###
### Set proper environment variables
$ mkdir ${HOME}/project
$ cd ${HOME}/project
$ git clone https://github.com/jkjung-avt/jetson_nano.git
$ cd jetson_nano
$ ./install_basics.sh
$ source ${HOME}/.bashrc
```

# 2. Making sure python3 "cv2" is working

I'd just use the built-in OpenCV-4.1.1 provided by NVIDIA (no need to build from source by myself).  I'd do the following for system library dependencies and python module dependencies.  **NOTE:** For "protobuf" libraries, instead of doing apt install, I recommend installing the newer version (3.8.0) using my "install_protobuf-3.8.0.sh" script as shown below.  The "protobuf" libraries could have a noticeable effect on the performance (inference speed) of tensorflow or else.  The building/installation process of "protobuf" could take a couple of hours, though.  So be patient...

In addition, I use an older version of pip3, "20.2.1", since the latest version does not work (SSLError: "WRONG_VERSION_NUMBER", which seems related to accessing https pip index url's behind a firewall/proxy) in our company lab network environment.

```shell
### Install dependencies for python3 "cv2"
$ sudo apt update
$ sudo apt install -y build-essential make cmake cmake-curses-gui \
                      git g++ pkg-config curl libfreetype6-dev \
                      libcanberra-gtk-module libcanberra-gtk3-module \
                      python3-dev python3-pip
$ sudo pip3 install -U pip==20.2.1 Cython testresources setuptools
$ cd ${HOME}/project/jetson_nano
$ ./install_protobuf-3.8.0.sh
$ sudo pip3 install numpy==1.16.1 matplotlib==3.2.2
```

Then I'd test my [tegra-cam.py](https://gist.github.com/jkjung-avt/86b60a7723b97da19f7bfa3cb7d2690e) script with a USB webcam, and make sure the python3 "cv2" module could capture and display images properly.

```shell
### Test tegra-cam.py (using a USB webcam)
$ cd ${HOME}/project
$ wget https://gist.githubusercontent.com/jkjung-avt/86b60a7723b97da19f7bfa3cb7d2690e/raw/3dd82662f6b4584c58ba81ecba93dd6f52c3366c/tegra-cam.py
$ python3 tegra-cam.py --usb --vid 0
```

# 3. Installing tensorflow-2.3.1

Reference (official documentation from NVIDIA): [Installing TensorFlow For Jetson Platform](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html)

To install tensorflow, I just followed instructions on the official documentation, but skipped installation of "protobuf".  (I already built and installed "protobuf-3.8.0" in the opencv section.)  These steps would take roughly 20~30 minutes to finish.

```shell
$ sudo apt install -y libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev \
                      zip libjpeg8-dev liblapack-dev libblas-dev gfortran
$ sudo pip3 install -U numpy==1.16.1 future==0.18.2 mock==3.0.5 h5py==2.10.0 \
                       keras_preprocessing==1.1.1 keras_applications==1.0.8 \
                       gast==0.2.2 futures pybind11
$ sudo pip3 install --pre --extra-index-url \
                    https://developer.download.nvidia.com/compute/redist/jp/v45 \
                    tensorflow==2.3.1
```

I tested and made sure "import tensorflow as tf" worked OK in python3.

# 4. Testing TensorRT GoogLeNet and MTCNN

Please refer to my [JetPack-4.4 for Jetson Nano](https://jkjung-avt.github.io/jetpack-4.4/) post if you are interested in testing these 2 models.

# 5. Skipping TensorRT SSD models

My [Demo #3: SSD](https://github.com/jkjung-avt/tensorrt_demos#demo-3-ssd) example only works against tensorflow-1.x.  So I skipped this testing.

# 6. Testing TensorRT YOLOv3 and YOLOv4 models

Reference: [Demo #4: YOLOv3](https://github.com/jkjung-avt/tensorrt_demos#demo-4-yolov3) and [Demo #5: YOLOv4](https://github.com/jkjung-avt/tensorrt_demos#yolov4)

I installed dependencies and build the TensorRT yolov3/yolov4 engines.

```shell
### Install dependencies and build TensorRT engine
$ cd ${HOME}/project/tensorrt_demos/ssd
$ ./install_pycuda.sh
$ sudo pip3 install onnx==1.4.1
$ cd ${HOME}/project/tensorrt_demos/plugins
$ make
$ cd ${HOME}/project/tensorrt_demos/yolo
$ ./download_yolo.sh
$ python3 yolo_to_onnx.py -m yolov3-416
$ python3 onnx_to_tensorrt.py -v -m yolov3-416
$ python3 yolo_to_onnx.py -m yolov4-416
$ python3 onnx_to_tensorrt.py -v -m yolov4-416
```

Next, I tested the TensorRT engines with the USB webcam.

```shell
$ cd ${HOME}/project/tensorrt_demos
$ python3 trt_yolo.py --usb 0 -m yolov3-416
$ python3 trt_yolo.py --usb 0 -m yolov4-416
```

# Conclusion

JetPack-4.5 contains exactly the same versions of CUDA toolkit, cuDNN and TensorRT libraries as JetPack-4.4.  I didn't feel any performance difference between JetPack-4.5 and the previous version when testing it on Jetson Nano DevKit.

According to [NVIDIA's announcement](https://forums.developer.nvidia.com/t/jetpack-4-5-production-release-with-l4t-32-5/166475), one particular new addition in JetPack-4.5 is availability of JetPack 4.5 container on NGC.  I probably should find time to test that.
