---
layout: post
comments: true
title: "YOLOv2 on Jetson TX2"
excerpt: "I tested pre-trained YOLOv2 on Jetson TX2, and documented the result in the post."
date: 2017-11-12
category: "yolo"
tags: yolo
---

I have been working extensively on deep-learning based object detection techniques in the past few weeks. While I have spent quite some time learning, training and testing [NVIDIA DetectNet](https://devblogs.nvidia.com/parallelforall/detectnet-deep-neural-network-object-detection-digits/) and [Fater RCNN](https://github.com/rbgirshick/py-faster-rcnn) models, I did also check out how [YOLOv2](https://pjreddie.com/darknet/yolo/) performs on Jetson TX2. Here are the results.

YOLOv2 is an improved version of "You Only Look Once (YOLO)". To gain an understanding about how YOLO/YOLOv2 works, I recommend reading the orginal papers along with the following video presentations by the orginal author.

* ["You Only Look Once: Unified, Real-Time Object Detection"](https://arxiv.org/abs/1506.02640)
* ["YOLO9000: Better, Faster, Stronger"](https://arxiv.org/abs/1612.08242), aka YOLOv2
* [Video: Presentation of YOLO at CVPR 2016](https://youtu.be/NM6lrxy0bxs)
* [Video: Presentation of YOLO 9000](https://youtu.be/GBu2jofRJtk)

In a nutshell, YOLOv2 incorporates the following improvements over the original YOLO to achieve an impressive 15.2 points of increase in mAP on Pascal VOC 2007 dataset.

![YOLOv2 improvements](/assets/2017-11-12-yolov2/YOLOv2_improvements.jpg)

Running pre-trained YOLOv2 models on Jetson TX2 is pretty straightforward. I mainly just followed instructions on the official YOLOv2 (Darknet) website:

[YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/)

Here is how I've done it:

1. Clone the latest darknet source code from GitHub.

   ```shell
   $ cd ~/project 
   $ git clone https://github.com/pjreddie/darknet
   $ cd darknet
   ```

2. Modify the first few lines of `Makefile` as follows. Note that CUDA architecture of TX2 is "62", while TX1 "53".

   ```
   GPU=1
   CUDNN=1
   OPENCV=1
   ......
   ARCH= -gencode arch=compute_50,code=[sm_50,compute_50] \
         -gencode arch=compute_53,code=[sm_53,compute_53] \
         -gencode arch=compute_62,code=[sm_62,compute_62]
   ```

3. Build the code.

   ```shell
   $ make
   ```

4. To test YOLOv2 with live video feed, I used a USB webcam (`/dev/video1`). I saw YOLOv2 only processed 2~3 frames per second. This was with the regular (larger) YOLOv2 model. And note that I had set Jetson TX2 to maximum performance mode with `nvpmodel` and `~/jetson_clocks.sh`.

   ```shell
   $ wget https://pjreddie.com/media/files/yolo.weights
   $ ./darknet detector demo cfg/coco.data cfg/yolo.cfg yolo.weights -c 1
   ```

5. I also tested the **Tiny YOLO model**. The result was much better. I was able to get **16~17 frames per second** processed and displayed!

   ```
   $ wget https://pjreddie.com/media/files/tiny-yolo-voc.weights
   $ ./darknet detector demo cfg/voc.data cfg/tiny-yolo-voc.cfg tiny-yolo-voc.weights -c 1
   ```

   ![Testing Tiny YOLO on Jetson TX2](/assets/2017-11-12-yolov2/tiny-yolo-voc.png)

Overall I think the speed of Tiny YOLOv2 is very good comparing to Faster RCNN. However, YOLOv2 is not implemented with Caffe, and it's difficult for me to evaluate its accuracy/mAP (for the datasets/tasks I really care about). So for now I'm still sticking to Faster RCNN for my work.
