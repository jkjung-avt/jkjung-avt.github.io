---
layout: post
comments: true
title: "JetPack-4.4 for Jetson Nano"
excerpt: "This post summarizes the steps I applied to set up my Jetson Nano DevKit with JetPack-4.4 and run my tensorrt_demos samples."
date: 2020-05-15
category: "nano"
tags: nano
---

I wrote about [JetPack-4.3](https://jkjung-avt.github.io/jetpack-4.3/) soon after it was released late last year.  This post is about JetPack-4.4 and is going to be similar to the previous one.

At the time of this writing, JetPack-4.4 is still a "Developer Preview (DP)" release.  I plan to update this post once the "General Availability (GA)" version is formally released.  So feel free to check back on this post later.

You could refer to [this announcement](https://forums.developer.nvidia.com/t/jetpack-4-4-developer-preview-l4t-r32-4-2-released/120594) for a clear summary of NVIDIA software updates in JetPack-4.4.  From my own point of view, the most significant updates (comparing to JetPack-4.3) are **CUDA 10.2, cuDNN 8 and TensorRT 7** (previously CUDA 10.0, cuDNN 7 and TensorRT 6).  Upgrades of these libraries might result in better CNN/DNN inferencing performance on the Jetson platforms.

I break down my steps of setting up and testing JetPack-4.4 on Jetson Nano as follows.

# 1. Basic set-up

Reference: [Setting up Jetson Nano: The Basics](https://jkjung-avt.github.io/setting-up-nano/)

I downloaded the JetPack-4.4 DP image, [nv-jetson-nano-sd-card-image-r32.4.2.zip](https://developer.nvidia.com/jetson-nano-sd-card-image), and [etched the image onto a new Micro SD card](https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#write).  After booting up Jetson Nano with the image, I finished Ubuntu set-up process and made sure internet connection was working OK.

> Tip #1: When Jetson Nano boot up to Desktop the first time, I would hit Ctrl-Alt-T to bring up a terminal and then lock the terminal on the Launcher (on the left-hand side).

> Tip #2: In the command terminal, I would type and execute "chromium-browser" to bring up the web browser.  Again, I would lock it onto Launcher for easier use later on.

Then I ran the following commands to do basic set-up of the system.

```shell
$ sudo apt update
###
### Set proper environment variables
$ mkdir ${HOME}/project
$ cd ${HOME}/project
$ git clone https://github.com/jkjung-avt/jetson_nano.git
$ cd jetson_nano
$ ./install_basics.sh
$ source ${HOME}/.bashrc
###
### Enable swap
$ sudo fallocate -l 8G /mnt/8GB.swap
$ sudo mkswap /mnt/8GB.swap
$ sudo swapon /mnt/8GB.swap
$ if ! grep swap /etc/fstab > /dev/null; then \
    echo "/mnt/8GB.swap  none  swap  sw  0  0" | sudo tee -a /etc/fstab; \
  fi
```

# 2. Making sure python3 "cv2" is working

I used the built-in OpenCV-4.1.1 in JetPack-4.4.  I did the following for system library dependencies and python module dependencies.  **NOTE:** For "protobuf" libraries, instead of doing apt install, I recommend installing the newer version (3.8.0) using my "install_protobuf-3.8.0.sh" script as shown below.  The "protobuf" libraries could have a noticeable effect on the performance (inference speed) of tensorflow or else.

```shell
### Install dependencies for python3 "cv2"
$ sudo apt-get update
$ sudo apt-get install -y build-essential make cmake cmake-curses-gui
$ sudo apt-get install -y git g++ pkg-config curl libfreetype6-dev
$ sudo apt-get install -y libcanberra-gtk-module libcanberra-gtk3-module
$ sudo apt-get install -y python3-dev python3-testresources python3-pip
$ sudo pip3 install -U pip
$ cd ${HOME}/project/jetson_nano
$ ./install_protobuf-3.8.0.sh
$ sudo pip3 install numpy matplotlib
```

Then I tested my [tegra-cam.py](https://gist.github.com/jkjung-avt/86b60a7723b97da19f7bfa3cb7d2690e) script with a USB webcam, and made sure the python3 "cv2" module could capture and display images properly.

```shell
### Test tegra-cam.py (using a USB webcam)
$ cd ${HOME}/project
$ wget https://gist.githubusercontent.com/jkjung-avt/86b60a7723b97da19f7bfa3cb7d2690e/raw/3dd82662f6b4584c58ba81ecba93dd6f52c3366c/tegra-cam.py
$ python3 tegra-cam.py --usb --vid 0
```

# 3. Installing tensorflow-1.15.2

Reference (official documentation from NVIDIA): [Installing TensorFlow For Jetson Platform](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html)

At the time of this writing, NVIDIA has provided pip wheel files for both tensorflow-1.15.2 and tensorflow-2.1.0 ([link](https://developer.download.nvidia.com/compute/redist/jp/v44/tensorflow/)).  I used 1.15.2 since my TensorRT [Demo #3: SSD](https://github.com/jkjung-avt/tensorrt_demos#demo-3-ssd) only works for tensorflow-1.x.

To install tensorflow, I just followed instructions on the official documentation, but skipped installation of "protobuf".  (I already built and installed "protobuf-3.8.0" in the opencv section.)

```shell
$ sudo apt-get install -y libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran
$ sudo pip3 install -U pip testresources setuptools
$ sudo pip3 install -U numpy==1.16.1 future mock h5py keras_preprocessing keras_applications gast futures pybind11
$ sudo pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v44 tensorflow==1.15.2
```

At this point, I tested and made sure "import tensorflow as tf" worked OK in python3.

# 4. Testing TensorRT GoogLeNet and MTCNN

Reference: [Demo #1: GoogLeNet](https://github.com/jkjung-avt/tensorrt_demos#googlenet) and [Demo #2: MTCNN](https://github.com/jkjung-avt/tensorrt_demos#demo-2-mtcnn)

```shell
### Clone the tensorrt_demos code
$ cd ${HOME}/project
$ git clone https://github.com/jkjung-avt/tensorrt_demos.git
###
### Build TensorRT engine for GoogLeNet
$ cd ${HOME}/project/tensorrt_demos/googlenet
$ make
$ ./create_engine
###
### Build TensorRT engines for MTCNN
$ cd ${HOME}/project/tensorrt_demos/mtcnn
$ make
$ ./create_engines
###
### Build the "pytrt" Cython module
$ cd ${HOME}/project/tensorrt_demos
$ sudo pip3 install Cython
$ make
```

Test TensorRT GoogLeNet.

```shell
$ python3 trt_googlenet.py --usb --vid 0
```

Test TensorRT MTCNN.

```shell
$ python3 trt_mtcnn.py --usb --vid 0
```

# 5. Testing TensorRT UFF SSD models

Reference: [Demo #3: SSD](https://github.com/jkjung-avt/tensorrt_demos#demo-3-ssd)

```shell
### Install dependencies and build TensorRT engines
$ cd ${HOME}/project/tensorrt_demos/ssd
$ ./install.sh
$ ./build_engines.sh
```

Test TensorRT SSD models.

```shell
$ cd ${HOME}/project/tensorrt_demos
$ python3 trt_ssd_async.py --model ssd_mobilenet_v1_coco --usb --vid 0
$ python3 trt_ssd_async.py --model ssd_mobilenet_v2_coco --usb --vid 0
```

# 6. Testing TensorRT ONNX YOLOv3 model

Reference: [Demo #4: YOLOv3](https://github.com/jkjung-avt/tensorrt_demos#demo-4-yolov3)

```shell
### Install dependencies and build TensorRT engine
$ sudo pip3 install onnx==1.4.1
$ cd ${HOME}/project/tensorrt_demos/yolov3_onnx
$ ./download_yolov3.sh
$ python3 yolov3_to_onnx.py --model yolov3-416
$ python3 onnx_to_tensorrt.py --model yolov3-416
```

Test TensorRT YOLOv3 (416x416) model.

```shell
$ cd ${HOME}/project/tensorrt_demos
$ python3 trt_yolov3.py --model yolov3-416 --usb --vid 0
```

# Test results

Overall, I was pretty happy to see that my [tensorrt_demos](https://github.com/jkjung-avt/tensorrt_demos) worked on JetPack-4.4 almost without any modifications.  Although NVIDIA has announced that Caffe Parser and UFF Parser were deprecated since [TensorRT 7.0.0 release](https://docs.nvidia.com/deeplearning/sdk/tensorrt-release-notes/tensorrt-7.html#rel_7-0-0), my TensorRT GoogLeNet (Caffe), MTCNN (Caffe) and SSD (UFF) code were still working.

One other important goal of my testing was to see whether CNN inference speed was improved with this new JetPack release (newer versions of CUDA, cuDNN and TensorRT).  Based on my current observation, FPS (frames per second) numbers of my TensorRT GoogLeNet, MTCNN and SSD models were roughly the same as before (JetPack-4.3).  But I did see better FPS number for the TensorRT YOLOv3-416 model (3.17 -> 3.75).

My preliminary guess is maybe TensorRT 7 could improve inferencing performance for "larger models" (but not so much for the smaller models?).  I'd share more if I have more findings later on.
